# ğŸ“Š Sampling Assignment â€” Credit Card Fraud Detection

This repository contains a Python implementation to explore how different **sampling techniques** affect the performance of various **machine learning classifiers** on an imbalanced credit card fraud dataset.

Imbalanced data is common in fraud detection problems. Without proper handling, models tend to predict only the majority (legitimate) class, failing to detect the rare but important fraudulent transactions. This project balances the data and compares multiple sampling strategies and models.

---

## ğŸ” Repository Contents

Sampling_assignment/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ Creditcard_data.csv
â”œâ”€â”€ sampling_assignment.py
â”œâ”€â”€ results/
â”‚ â””â”€â”€ accuracy_table.csv
â”œâ”€â”€ README.md


---

## ğŸ§  Project Goals

1. Handle class imbalance via undersampling  
2. Implement multiple sampling techniques  
3. Train five different classifiers  
4. Compare model performance across sampling methods  
5. Identify which sampling strategy works best per model  

---

## ğŸ“Š Dataset

- File path: `data/Creditcard_data.csv`  
- Target Column: `Class`

| Value | Meaning |
|------:|--------|
| 0 | Legitimate transaction |
| 1 | Fraudulent transaction |

---

## ğŸ§ª Sampling Techniques Used

| Technique | Description |
|---------|-------------|
| Random | Random sampling without replacement |
| Systematic | Picks every k-th row after shuffling |
| Stratified | Preserves class ratios |
| Bootstrap | Sampling with replacement |
| CrossVal | Uses training fold of K-Fold |

---

## ğŸ§  Machine Learning Models

| Code | Model |
|-----|------|
| M1 | Logistic Regression |
| M2 | Random Forest |
| M3 | K-Nearest Neighbors |
| M4 | Support Vector Machine |
| M5 | Decision Tree |

---

## ğŸš€ How It Works (Step-by-Step)

---

1ï¸âƒ£ Load and Balance Dataset

The script loads the CSV dataset, then uses undersampling to balance the number of fraud and non-fraud records.

df_bal = pd.concat([majority_down, minority])

2ï¸âƒ£ Apply Sampling Methods

Each sampling method generates a new subset of the balanced dataset for model training.

Example â€” Random Sampling

idx = np.random.choice(len(X), sample_size, replace=False)
Xs = X.iloc[idx]
ys = y.iloc[idx]

3ï¸âƒ£ Train Models and Evaluate

Each sampled dataset is split into training and test sets (80/20).

Models are trained on the training set and accuracy is recorded on the test set.

model.fit(Xtr, ytr)
accuracy_score(yte, pred)

ğŸ§¾ Results

The script prints and saves a CSV file showing accuracy scores for each model on each sampling method.

           Random  Systematic  Stratified  Bootstrap  CrossVal
M1         85.23        65.10        88.45       81.65      87.12
M2         90.12        70.98        92.45       85.34      91.80
...


The best sampling method is also determined for each model based on highest accuracy.

ğŸ“ Saved Output
results/accuracy_table.csv